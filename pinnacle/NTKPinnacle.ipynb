{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce80ba17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7bc0a892e630>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from typing import Dict, Any, Tuple, Union, NamedTuple\n",
    "import hydra\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "lr = 1e-3\n",
    "max_steps = 12000\n",
    "print_freq = 100\n",
    "\n",
    "\n",
    "torch.manual_seed(995) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8377fe65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lc = 1e-9\n",
    "cc = 1e-5\n",
    "# Physical constants\n",
    "F = 96485 # Faraday constant [C/mol]\n",
    "R = 8.3145 # Gas constant [J/(mol·K)]\n",
    "T = 293 # Temperature [K]\n",
    "k_B = 1.3806e-23 # Boltzmann constant [J/K]\n",
    "eps0 = 8.85e-12 # Vacuum permittivity [F/m]\n",
    "E_min = -1.0  # External applied potential [V] \n",
    "E_max = 1.8\n",
    "# Diffusion coefficients [m²/s]\n",
    "D_cv = 1.0e-21\n",
    "D_av = 1.0e-21\n",
    "D_h = 3.2823e-4\n",
    "\n",
    "# Mobility coefficients [m²/(V·s)]\n",
    "U_cv = -1.0562e-19\n",
    "U_av = 7.9212e-20\n",
    "U_h = 0.013 # mo_h from COMSOL\n",
    "\n",
    "# Species charges\n",
    "z_cv = -2.6667 # -8/3\n",
    "z_av = 2\n",
    "z_h = 1\n",
    "\n",
    "# Permittivities [F/m]\n",
    "epsilonf = 1.239e-10 # 14*eps0\n",
    "eps_film = 1.239e-10 # Same as epsilonf\n",
    "eps_Ddl = 1.77e-11 # 2*eps0\n",
    "eps_dl = 6.947e-10 # 78.5*eps0\n",
    "eps_sol = 6.947e-10 # Same as eps_dl\n",
    "\n",
    "# Semiconductor properties\n",
    "c_h0 = 4.1683e-4 # Intrinsic hole concentration [mol/m³]\n",
    "c_e0 = 9.5329e-28 # Intrinsic electron concentration [mol/m³]\n",
    "tau = 4.9817e-13 # Recombination time constant [s·mol/m³]\n",
    "Nc = 166.06 # Conduction band density [mol/m³]\n",
    "Nv = 1.6606e5 # Valence band density [mol/m³]\n",
    "mu_e0 = 2.4033e-19 # Standard electron chemical potential [J]\n",
    "Ec0 = 5.127e-19 # Conduction band edge [J]\n",
    "Ev0 = 1.6022e-19 # Valence band edge [J]\n",
    "\n",
    "# Solution properties\n",
    "c_H = 0.01 # Proton concentration [mol/m³]\n",
    "pH = 5\n",
    "\n",
    "# Molar volume\n",
    "Omega = 1.4e-5 # [m³/mol]\n",
    "# Standard rate constants\n",
    "k1_0 = 4.5e-8 # [m/s]\n",
    "k2_0 = 3.6e-6 # [mol/(m²·s)]\n",
    "k3_0 = 4.5e-9 # [mol/(m²·s)]\n",
    "k4_0 = 2.25e-7 # [m/s]\n",
    "k5_0 = 7.65e-9 # [mol/(m²·s)]\n",
    "ktp_0 = 4.5e-8 # [-]\n",
    "ko2_0 = 0.005 # [m/s]\n",
    "\n",
    "# Charge transfer coefficients\n",
    "alpha_cv = 0.3\n",
    "alpha_av = 0.8\n",
    "beta_cv = 0.1\n",
    "beta_av = 0.8\n",
    "alpha_tp = 0.2\n",
    "a_par = 0.45 # For oxygen evolution\n",
    "delta3 = 1.0\n",
    "\n",
    "# Derived parameters [1/V]\n",
    "a_cv = 23.764 # alpha_cv * 2 * F/(R*T)\n",
    "a_av = 84.493 # alpha_av * 8/3 * F/(R*T)\n",
    "b_cv = 7.9212 # beta_cv * 2 * F/(R*T)\n",
    "\n",
    "# Geometric parameters [m]\n",
    "d_Ddl = 2.0e-10 # Defect layer thickness\n",
    "d_dl = 5.0e-10 # Double layer thickness\n",
    "L_cell = 1.0e-6 # Cell length\n",
    "\n",
    "# Equilibrium potentials\n",
    "phi_O2_eq = 1.35 # [V]\n",
    "phic = (R*T)/F\n",
    "tc = (lc ** 2) / D_cv\n",
    "L_initial = 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c9a40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define networks\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return torch.sigmoid(x)*x\n",
    "    \n",
    "\n",
    "class FFN(nn.Module):\n",
    "    \"\"\"\n",
    "    Fully Connected Feed Forward Neural Network.\n",
    "    Args:\n",
    "        input_dim: Number of input features\n",
    "        output_dim: Number of output features  \n",
    "        hidden_layers: Number of hidden layers\n",
    "        layer_size: Size of each hidden layer\n",
    "        activation: Activation function name ('swish', 'swoosh', 'swash', 'squash_swish', 'relu', 'tanh')\n",
    "        initialize_weights: Whether to apply Xavier initialization\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int = 3,\n",
    "        output_dim: int = 1,\n",
    "        hidden_layers: int = 5,\n",
    "        layer_size: int = 20,\n",
    "        activation: str = \"swish\",\n",
    "        initialize_weights: bool = False\n",
    "    ):\n",
    "        super(FFN, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = hidden_layers\n",
    "        self.layer_size = layer_size\n",
    "        self.activation = Swish()\n",
    "        \n",
    "        # Input layer\n",
    "        self.input_layer = nn.Linear(input_dim, self.layer_size)\n",
    "        \n",
    "        # Hidden layers\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            nn.Linear(self.layer_size, self.layer_size)\n",
    "            for _ in range(self.num_layers)  \n",
    "        ])\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(self.layer_size, output_dim)\n",
    "        \n",
    "        # Initialize weights\n",
    "        if initialize_weights:\n",
    "            self.initialize_weights()\n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        \"\"\"Apply Xavier initialization to all linear layers\"\"\"\n",
    "        # Initialize input layer\n",
    "        nn.init.xavier_normal_(self.input_layer.weight)\n",
    "        nn.init.zeros_(self.input_layer.bias)\n",
    "        \n",
    "        # Initialize hidden layers\n",
    "        for layer in self.hidden_layers:\n",
    "            nn.init.xavier_normal_(layer.weight)\n",
    "            nn.init.zeros_(layer.bias)\n",
    "        \n",
    "        # Initialize output layer\n",
    "        nn.init.xavier_normal_(self.output_layer.weight)\n",
    "        nn.init.zeros_(self.output_layer.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.input_layer(x))\n",
    "        \n",
    "        for layer in self.hidden_layers: \n",
    "            x = self.activation(layer(x))\n",
    "        \n",
    "        return self.output_layer(x)\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Single residual block: x + F(x)\"\"\"\n",
    "    def __init__(self, layer_size, activation):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.layer_size = layer_size\n",
    "        self.activation = activation\n",
    "        \n",
    "        # Two layers in each residual block\n",
    "        self.linear1 = nn.Linear(layer_size, layer_size)\n",
    "        self.linear2 = nn.Linear(layer_size, layer_size)\n",
    "        \n",
    "    def initialize_weights(self):\n",
    "        nn.init.xavier_normal_(self.linear1.weight)\n",
    "        nn.init.zeros_(self.linear1.bias)\n",
    "        nn.init.xavier_normal_(self.linear2.weight)\n",
    "        nn.init.zeros_(self.linear2.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = x  # Save input for residual connection\n",
    "        \n",
    "        # F(x) computation\n",
    "        out = self.activation(self.linear1(x))\n",
    "        out = self.linear2(out)  # No activation on final layer of block\n",
    "        \n",
    "        # Residual connection: x + F(x)\n",
    "        out = out + identity\n",
    "        \n",
    "        # Activation after residual connection\n",
    "        out = self.activation(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class ResidualFFN(nn.Module):\n",
    "    def __init__(self, input_dim=3, output_dim=1, num_layers=8, layer_size=50, initialize_weights=True):\n",
    "        super(ResidualFFN, self).__init__()\n",
    "        self.layer_size = layer_size\n",
    "        self.num_layers = num_layers\n",
    "        self.activation = Swish()\n",
    "        \n",
    "        # Input projection to get to residual dimension\n",
    "        self.input_layer = nn.Linear(input_dim, self.layer_size)\n",
    "        \n",
    "        # Residual blocks\n",
    "        self.residual_layers = nn.ModuleList([\n",
    "            ResidualBlock(self.layer_size, self.activation)\n",
    "            for _ in range(self.num_layers)  \n",
    "        ])\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(self.layer_size, output_dim)\n",
    "        \n",
    "        if initialize_weights:\n",
    "            self.initialize_weights()\n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        \"\"\"Apply Xavier initialization to all linear layers\"\"\"\n",
    "        nn.init.xavier_normal_(self.input_layer.weight)\n",
    "        nn.init.zeros_(self.input_layer.bias)\n",
    "        \n",
    "        for block in self.residual_layers:\n",
    "            block.initialize_weights()\n",
    "        \n",
    "        nn.init.xavier_normal_(self.output_layer.weight)\n",
    "        nn.init.zeros_(self.output_layer.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input projection\n",
    "        x = self.activation(self.input_layer(x))\n",
    "        \n",
    "        # Residual blocks\n",
    "        for residual_layer in self.residual_layers:\n",
    "            x = residual_layer(x)\n",
    "        \n",
    "        # Output\n",
    "        return self.output_layer(x)\n",
    "    \n",
    "\"\"\"\n",
    "cv_net = ResidualFFN(input_dim=3, output_dim=1, num_layers=3, layer_size=20) #ResNet Size as close to old FFN as possible. There is one extra layer here by nature of design.\n",
    "av_net = ResidualFFN(input_dim=3, output_dim=1, num_layers=3, layer_size=20)\n",
    "u_net = ResidualFFN(input_dim=3, output_dim=1, num_layers=3, layer_size=20)\n",
    "L_net = ResidualFFN(input_dim=2, output_dim=1, num_layers=3, layer_size=20)\n",
    "\"\"\"\n",
    "cv_net = FFN()\n",
    "av_net = FFN()\n",
    "u_net = FFN()\n",
    "L_net = FFN(2,1)\n",
    "cv_net.to(device)\n",
    "av_net.to(device)\n",
    "u_net.to(device)\n",
    "L_net.to(device)\n",
    "\n",
    "total_model_parameters = list(cv_net.parameters()) + list(av_net.parameters()) + list(u_net.parameters()) + list(L_net.parameters()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5c53c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradient and sampling utils\n",
    "\n",
    "class GradientResults(NamedTuple):\n",
    "    \"\"\"\n",
    "    Container for gradient computation results.\n",
    "\n",
    "    Organizes all computed derivatives in a structured way for easy access.\n",
    "    \"\"\"\n",
    "    # Network predictions\n",
    "    phi: torch.Tensor  # Potential φ\n",
    "    c_cv: torch.Tensor  # Cation vacancy concentration\n",
    "    c_av: torch.Tensor  # Anion vacancy concentration\n",
    "\n",
    "    # Time derivatives\n",
    "    c_cv_t: torch.Tensor  # ∂c_cv/∂t\n",
    "    c_av_t: torch.Tensor  # ∂c_av/∂t\n",
    "\n",
    "    # First spatial derivatives\n",
    "    phi_x: torch.Tensor  # ∂φ/∂x\n",
    "    c_cv_x: torch.Tensor  # ∂c_cv/∂x\n",
    "    c_av_x: torch.Tensor  # ∂c_av/∂x\n",
    "\n",
    "    # Second spatial derivatives\n",
    "    phi_xx: torch.Tensor  # ∂²φ/∂x²\n",
    "    c_cv_xx: torch.Tensor  # ∂²c_cv/∂x²\n",
    "    c_av_xx: torch.Tensor  # ∂²c_av/∂x²\n",
    "\n",
    "\n",
    "def _grad(x,dx):\n",
    "    \"\"\"Take the derrivative of x w.r.t dx\"\"\"\n",
    "\n",
    "    return torch.autograd.grad(x,dx,torch.ones_like(dx),create_graph=True,retain_graph=True)[0]\n",
    "\n",
    "def compute_gradients(x, t, E):\n",
    "    inputs_3d = torch.cat([x, t, E], dim=1)\n",
    "\n",
    "    # Get network predictions\n",
    "    phi = u_net(inputs_3d)\n",
    "    c_cv_raw = cv_net(inputs_3d)\n",
    "    c_av_raw = av_net(inputs_3d)\n",
    "\n",
    "\n",
    "    # Networks predict concentrations directly\n",
    "    c_cv = c_cv_raw\n",
    "    c_av = c_av_raw\n",
    "\n",
    "\n",
    "    # Direct derivatives\n",
    "    c_cv_t = _grad(c_cv, t)\n",
    "    c_av_t = _grad(c_av, t)\n",
    "\n",
    "    # Compute first spatial derivatives\n",
    "    phi_x = _grad(phi, x)\n",
    "\n",
    "    c_cv_x = _grad(c_cv, x)\n",
    "    c_av_x = _grad(c_av, x)\n",
    "\n",
    "    # Compute second spatial derivatives\n",
    "    phi_xx = _grad(phi_x, x)\n",
    "\n",
    "    c_cv_xx = _grad(c_cv_x, x)\n",
    "    c_av_xx = _grad(c_av_x, x)\n",
    "\n",
    "    return GradientResults(\n",
    "        phi=phi, c_cv=c_cv, c_av=c_av,\n",
    "        c_cv_t=c_cv_t, c_av_t=c_av_t,\n",
    "        phi_x=phi_x, c_cv_x=c_cv_x, c_av_x=c_av_x, \n",
    "        phi_xx=phi_xx, c_cv_xx=c_cv_xx, c_av_xx=c_av_xx\n",
    "    )\n",
    "\n",
    "def sample_interior_points(\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Sample interior collocation points for PDE residuals.\n",
    "\n",
    "        Args:\n",
    "            networks: NetworkManager instance\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (x, t, E) tensors with requires_grad=True for x and t\n",
    "        \"\"\"\n",
    "        batch_size = 2048\n",
    "\n",
    "        # Sample time and applied potential\n",
    "        t = torch.rand(batch_size, 1, device=device, requires_grad=True)\n",
    "        single_E = torch.tensor(0.8,device=device)\n",
    "        E = single_E.expand(batch_size, 1)\n",
    "\n",
    "        # Get film thickness prediction\n",
    "        L_pred = L_net(torch.cat([t, E], dim=1))\n",
    "\n",
    "        # Sample spatial coordinates within [0, L(t,E)]\n",
    "        x = torch.rand(batch_size, 1, device=device, requires_grad=True) * L_pred\n",
    "\n",
    "        return x, t, E\n",
    "\n",
    "def sample_boundary_points(\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Sample boundary collocation points for boundary conditions.\n",
    "\n",
    "    Args:\n",
    "        networks: NetworkManager instance\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (x, t, E) tensors for boundary points\n",
    "    \"\"\"\n",
    "    batch_size = 2 * 1024\n",
    "\n",
    "    # Sample time and applied potential\n",
    "    t = torch.rand(batch_size, 1, device=device, requires_grad=True)\n",
    "    single_E = torch.tensor(0.8,device=device)\n",
    "    E = single_E.expand(batch_size, 1)\n",
    "\n",
    "    # Predict L for f/s boundary\n",
    "    L_inputs = torch.cat([t, E], dim=1)\n",
    "    L_pred = L_net(L_inputs)\n",
    "\n",
    "    half_batch = batch_size // 2\n",
    "\n",
    "    # Metal/film interface points (x = 0)\n",
    "    x_mf = torch.zeros(half_batch, 1, device=device, requires_grad=True)\n",
    "    t_mf = t[:half_batch]\n",
    "    E_mf = E[:half_batch]\n",
    "\n",
    "    # Film/solution interface points (x = L)\n",
    "    x_fs = L_pred[half_batch:]\n",
    "    t_fs = t[half_batch:]\n",
    "    E_fs = E[half_batch:]\n",
    "\n",
    "    # Combine boundary points\n",
    "    x_boundary = torch.cat([x_mf, x_fs], dim=0)\n",
    "    t_boundary = torch.cat([t_mf, t_fs], dim=0)\n",
    "    E_boundary = torch.cat([E_mf, E_fs], dim=0)\n",
    "\n",
    "    return x_boundary, t_boundary, E_boundary\n",
    "\n",
    "def sample_initial_points(\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Sample initial condition points at t = 0.\n",
    "\n",
    "    Args:\n",
    "        networks: NetworkManager instance\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (x, t, E) tensors for initial condition points\n",
    "    \"\"\"\n",
    "    batch_size = 1024\n",
    "\n",
    "    # Initial time (t = 0)\n",
    "    t = torch.zeros(batch_size, 1, device=device, requires_grad=True)\n",
    "    single_E = torch.tensor(0.8,device=device)\n",
    "    E = single_E.expand(batch_size, 1)\n",
    "\n",
    "    # Get initial film thickness\n",
    "    L_initial_pred = L_net(torch.cat([t, E], dim=1))\n",
    "\n",
    "    # Sample spatial coordinates\n",
    "    x = (\n",
    "        torch.rand(batch_size, 1, device=device, requires_grad=True)\n",
    "        * L_initial_pred\n",
    "    )\n",
    "\n",
    "    return x, t, E\n",
    "\n",
    "def sample_film_physics_points() -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Sample points for film growth physics constraint.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (t, E) tensors for film physics constraint\n",
    "    \"\"\"\n",
    "    batch_size = 2048\n",
    "\n",
    "    # Sample time and applied potential\n",
    "    t = torch.rand(batch_size, 1, device=device, requires_grad=True)\n",
    "    single_E = torch.tensor(0.8,device=device)\n",
    "    E = single_E.expand(batch_size, 1)\n",
    "\n",
    "    return t, E\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75116210",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute and generate all the relevant losses\n",
    "\n",
    "def compute_rate_constants(t: torch.Tensor, E: torch.Tensor, single: bool = False):\n",
    "        \"\"\"\n",
    "        Compute electrochemical rate constants using Butler-Volmer kinetics.\n",
    "\n",
    "        **Butler-Volmer Rate Expressions:**\n",
    "\n",
    "        .. math::\n",
    "            \\\\hat{k}_{R1} = k_1^0 \\\\exp\\\\left(\\\\alpha_1 \\\\frac{3F\\\\hat{\\\\phi}_c}{RT}\\\\hat{\\\\phi}_{mf} \\\\right)\n",
    "\n",
    "        .. math::\n",
    "            \\\\hat{k}_{R2} = k_2^0 \\\\exp\\\\left(\\\\alpha_2 \\\\frac{2F\\\\hat{\\\\phi}_c}{RT}\\\\hat{\\\\phi}_{mf} \\\\right)\n",
    "\n",
    "        .. math::\n",
    "            \\\\hat{k}_{R3} = k_3^0 \\\\exp\\\\left(\\\\alpha_3 \\\\frac{(3-\\\\delta)F\\\\hat{\\\\phi}_c}{RT}\\\\hat{\\\\phi}_{fs} \\\\right)\n",
    "\n",
    "        .. math::\n",
    "            \\\\hat{k}_{R4} = k_4^0\n",
    "\n",
    "        .. math::\n",
    "            \\\\hat{k}_{R5} = k_5^0 (c_{H^+})^n\n",
    "\n",
    "        .. math::\n",
    "            \\\\hat{k}_{TP} = k_{tp}^0 \\\\hat{c}_h c_c \\\\exp\\\\left(\\\\alpha_{tp}\\\\frac{F\\\\hat{\\\\phi}_c}{RT}\\\\hat{\\\\phi}_{fs}\\\\right)\n",
    "\n",
    "        .. math::\n",
    "            \\\\hat{k}_{O2} = k_{o2}^0 \\\\exp\\\\left(\\\\alpha_{o2}\\\\frac{2F\\\\hat{\\\\phi}_c}{RT} \\\\left(\\\\hat{\\\\phi}_{ext} - \\\\hat{\\\\phi}_{o2,eq}\\\\right) \\\\right)\n",
    "\n",
    "        where:\n",
    "        - :math:`\\\\hat{\\\\phi}_{mf}` is the dimensionless potential at metal/film interface\n",
    "        - :math:`\\\\hat{\\\\phi}_{fs}` is the dimensionless potential at film/solution interface\n",
    "        - :math:`\\\\alpha_i, \\\\beta_i` are charge transfer coefficients\n",
    "\n",
    "        Args:\n",
    "            t: Time tensor (dimensionless)\n",
    "            E: Applied potential tensor\n",
    "            networks: NetworkManager instance\n",
    "            single: Whether computing for single point or batch\n",
    "\n",
    "        Returns:\n",
    "            Tuple of rate constants (k1, k2, k3, k4, k5, ktp, ko2)\n",
    "        \"\"\"\n",
    "        if single:\n",
    "            batch_size = 1\n",
    "            x_mf = torch.zeros(1, 1, device=device)\n",
    "        else:\n",
    "            batch_size = t.shape[0]\n",
    "            x_mf = torch.zeros(batch_size, 1, device=device)\n",
    "\n",
    "        # Get potentials at interfaces\n",
    "        inputs_mf = torch.cat([x_mf, t, E], dim=1)\n",
    "        u_mf = u_net(inputs_mf)  # φ̂_mf\n",
    "\n",
    "        # Get film thickness\n",
    "        L_inputs = torch.cat([t, E], dim=1)\n",
    "        L_pred = L_net(L_inputs)\n",
    "\n",
    "\n",
    "        x_fs = L_pred\n",
    "\n",
    "        inputs_fs = torch.cat([x_fs, t, E], dim=1)\n",
    "        u_fs = u_net(inputs_fs)  # φ̂_fs\n",
    "\n",
    "        # Compute rate constants using equations above\n",
    "        F_RT = F * phic / (R * T)\n",
    "\n",
    "        # k₁: Cation vacancy generation at m/f interface\n",
    "        k1 = k1_0 * torch.exp(alpha_cv * 3 * F_RT * u_mf)\n",
    "\n",
    "        # k₂: Anion vacancy generation at m/f interface\n",
    "        k2 = k2_0 * torch.exp(alpha_av * 2 * F_RT * u_mf)\n",
    "\n",
    "        # k₃: Cation vacancy consumption at f/s interface\n",
    "        k3 = k3_0 * torch.exp(beta_cv * (3 - delta3) * F_RT * u_fs)\n",
    "\n",
    "        # k₄: Chemical reaction (potential independent)\n",
    "        k4 = k4_0\n",
    "\n",
    "        # k₅: Chemical dissolution\n",
    "        k5 = k5_0 * c_H\n",
    "\n",
    "        return k1, k2, k3, k4, k5\n",
    "\n",
    "def compute_pde_residuals(x: torch.Tensor, t: torch.Tensor, E: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Compute PDE residuals for all governing equations.\n",
    "\n",
    "        **Cation Vacancy Conservation (Dimensionless Nernst-Planck):**\n",
    "\n",
    "        .. math::\n",
    "            \\\\frac{\\\\partial \\\\hat{c}_{cv}}{\\\\partial \\\\hat{t}} =\n",
    "            \\\\frac{D_{cv}\\\\hat{t}_c}{\\\\hat{L}_c^2}\\\\frac{\\\\partial^2 \\\\hat{c}_{cv}}{\\\\partial \\\\hat{x}^2} +\n",
    "            \\\\frac{U_{cv}\\\\hat{t}_c\\\\hat{\\\\phi}_c}{\\\\hat{L}_c^2}\\\\frac{\\\\partial \\\\hat{\\\\phi}}{\\\\partial \\\\hat{x}}\\\\frac{\\\\partial \\\\hat{c}_{cv}}{\\\\partial \\\\hat{x}} +\n",
    "            \\\\frac{U_{cv}\\\\hat{t}_c\\\\hat{\\\\phi}_c}{\\\\hat{L}_c^2}\\\\hat{c}_{cv}\\\\frac{\\\\partial^2 \\\\hat{\\\\phi}}{\\\\partial \\\\hat{x}^2}\n",
    "\n",
    "        **Anion Vacancy Conservation (Dimensionless Nernst-Planck):**\n",
    "\n",
    "        .. math::\n",
    "            \\\\frac{\\\\partial \\\\hat{c}_{av}}{\\\\partial \\\\hat{t}} =\n",
    "            \\\\frac{D_{av}\\\\hat{t}_c}{\\\\hat{L}_c^2}\\\\frac{\\\\partial^2 \\\\hat{c}_{av}}{\\\\partial \\\\hat{x}^2} +\n",
    "            \\\\frac{U_{av}\\\\hat{t}_c\\\\hat{\\\\phi}_c}{\\\\hat{L}_c^2}\\\\frac{\\\\partial \\\\hat{\\\\phi}}{\\\\partial \\\\hat{x}}\\\\frac{\\\\partial \\\\hat{c}_{av}}{\\\\partial \\\\hat{x}} +\n",
    "            \\\\frac{U_{av}\\\\hat{t}_c\\\\hat{\\\\phi}_c}{\\\\hat{L}_c^2}\\\\hat{c}_{av}\\\\frac{\\\\partial^2 \\\\hat{\\\\phi}}{\\\\partial \\\\hat{x}^2}\n",
    "\n",
    "        **Hole Conservation (Quasi-Steady State):**\n",
    "\n",
    "        .. math::\n",
    "            0 = \\\\frac{D_h\\\\hat{c}_{h,c}}{\\\\hat{L}_c^2}\\\\frac{\\\\partial^2 \\\\hat{c}_h}{\\\\partial \\\\hat{x}^2} +\n",
    "            \\\\frac{FD_h\\\\hat{\\\\phi}_c\\\\hat{c}_{h,c}}{RT\\\\hat{L}_c^2}\\\\frac{\\\\partial \\\\hat{\\\\phi}}{\\\\partial \\\\hat{x}}\\\\frac{\\\\partial \\\\hat{c}_h}{\\\\partial \\\\hat{x}} +\n",
    "            \\\\frac{FD_h\\\\hat{\\\\phi}_c\\\\hat{c}_{h,c}}{RT\\\\hat{L}_c^2}\\\\hat{c}_h\\\\frac{\\\\partial^2 \\\\hat{\\\\phi}}{\\\\partial \\\\hat{x}^2}\n",
    "\n",
    "        **Poisson's Equation (Dimensionless):**\n",
    "\n",
    "        .. math::\n",
    "            \\\\frac{\\\\partial^2 \\\\hat{\\\\phi}}{\\\\partial \\\\hat{x}^2} =\n",
    "            -\\\\frac{F\\\\hat{L}_c^2\\\\hat{c}_c}{\\\\hat{\\\\phi}_c\\\\varepsilon_f}\\\\left(z_{av}\\\\hat{c}_{av} + z_{cv}\\\\hat{c}_{cv}\\\\right)\n",
    "\n",
    "        Args:\n",
    "            x: Spatial coordinates (dimensionless)\n",
    "            t: Time coordinates (dimensionless)\n",
    "            E: Applied potential\n",
    "            networks: NetworkManager instance\n",
    "\n",
    "        Returns:\n",
    "            Tuple of residuals: (cv_residual, av_residual, h_residual, poisson_residual)\n",
    "        \"\"\"\n",
    "        # Get all gradients using the gradient computer\n",
    "        grads = compute_gradients(x, t, E)\n",
    "\n",
    "        # Cation vacancy residual - implements equation above\n",
    "        cv_residual = (grads.c_cv_t -\n",
    "                       (D_cv * tc / lc ** 2) * grads.c_cv_xx -\n",
    "                       (U_cv * tc * phic / lc ** 2) *\n",
    "                       grads.phi_x * grads.c_cv_x -\n",
    "                       (U_cv * tc * phic / lc ** 2) *\n",
    "                       grads.c_cv * grads.phi_xx)\n",
    "\n",
    "        # Anion vacancy residual - implements equation above\n",
    "        av_residual = (grads.c_av_t -\n",
    "                       (D_av * tc / lc ** 2) * grads.c_av_xx -\n",
    "                       (U_av * tc * phic / lc ** 2) *\n",
    "                       grads.phi_x * grads.c_av_x -\n",
    "                       (U_av * tc * phic / lc ** 2) *\n",
    "                       grads.c_av * grads.phi_xx)\n",
    "\n",
    "        # Poisson residual - implements equation above\n",
    "        poisson_residual = (grads.phi_xx +\n",
    "                            (F * lc ** 2 * cc /\n",
    "                             (phic * epsilonf)) *\n",
    "                            (z_av * grads.c_av + z_cv * grads.c_cv))\n",
    "\n",
    "        return cv_residual, av_residual, poisson_residual\n",
    "\n",
    "\n",
    "def compute_interior_loss(x: torch.Tensor, t: torch.Tensor, E: torch.Tensor,\n",
    "                          return_residuals: bool = False) -> Union[Tuple[torch.Tensor, Dict[str, torch.Tensor]],\n",
    "Tuple[torch.Tensor, Dict[str, torch.Tensor], Dict[str, torch.Tensor]]]:\n",
    "    \"\"\"\n",
    "    Compute interior PDE residual losses.\n",
    "\n",
    "    See compute_pde_residuals for mathematics of residual calculations\n",
    "\n",
    "    Args:\n",
    "        x: Spatial coordinates\n",
    "        t: Time coordinates\n",
    "        E: Applied potential\n",
    "        networks: NetworkManager instance\n",
    "        physics: ElectrochemicalPhysics instance\n",
    "        return_residuals: If True, also return raw residuals for NTK computation\n",
    "\n",
    "    Returns:\n",
    "        If return_residuals=False: Tuple of (total_interior_loss, individual_losses_dict)\n",
    "        If return_residuals=True: Tuple of (total_interior_loss, individual_losses_dict, residuals_dict)\n",
    "    \"\"\"\n",
    "    # Compute PDE residuals using physics module\n",
    "    cv_residual, av_residual, poisson_residual = compute_pde_residuals(x, t, E)\n",
    "\n",
    "    # Calculate individual losses\n",
    "    cv_pde_loss = torch.mean(cv_residual ** 2)\n",
    "    av_pde_loss = torch.mean(av_residual ** 2)\n",
    "    poisson_pde_loss = torch.mean(poisson_residual ** 2)\n",
    "\n",
    "    # Total interior loss\n",
    "    total_interior_loss = cv_pde_loss + av_pde_loss + poisson_pde_loss\n",
    "\n",
    "    individual_losses = {\n",
    "        'cv_pde': cv_pde_loss,\n",
    "        'av_pde': av_pde_loss,\n",
    "        'poisson_pde': poisson_pde_loss\n",
    "    }\n",
    "\n",
    "    if return_residuals:\n",
    "        residuals = {\n",
    "            'cv_pde': cv_residual,\n",
    "            'av_pde': av_residual,\n",
    "            'poisson_pde': poisson_residual\n",
    "        }\n",
    "        return total_interior_loss, individual_losses, residuals\n",
    "    else:\n",
    "        return total_interior_loss, individual_losses\n",
    "\n",
    "def compute_boundary_loss(x: torch.Tensor, t: torch.Tensor, E: torch.Tensor,\n",
    "                          return_residuals: bool = False) -> Union[Tuple[torch.Tensor, Dict[str, torch.Tensor]],\n",
    "Tuple[torch.Tensor, Dict[str, torch.Tensor], torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Compute boundary condition losses.\n",
    "     **Boundary Conditions:**\n",
    "\n",
    "    **Metal/Film Interface (x̂ = 0):**\n",
    "\n",
    "    *Cation Vacancy Flux:*\n",
    "\n",
    "    .. math::\n",
    "        -D_{cv}\\\\frac{\\\\partial \\\\hat{c}_{cv}}{\\\\partial \\\\hat{x}} = \\\\hat{k}_1 - \\\\left(U_{cv}\\\\frac{\\\\partial \\\\hat{\\\\phi}}{\\\\partial \\\\hat{x}} - \\\\frac{d\\\\hat{L}}{d\\\\hat{t}}\\\\right)\\\\hat{c}_{cv}\n",
    "\n",
    "    *Anion Vacancy Flux:*\n",
    "\n",
    "    .. math::\n",
    "        -D_{av}\\\\frac{\\\\partial \\\\hat{c}_{av}}{\\\\partial \\\\hat{x}} = \\\\frac{4}{3}\\\\hat{k}_2 + \\\\left(U_{av}\\\\frac{\\\\partial \\\\hat{\\\\phi}}{\\\\partial \\\\hat{x}} - \\\\frac{d\\\\hat{L}}{d\\\\hat{t}}\\\\right)\\\\hat{c}_{av}\n",
    "\n",
    "    *Potential Boundary:*\n",
    "\n",
    "    .. math::\n",
    "        \\\\varepsilon_f \\\\frac{\\\\partial \\\\hat{\\\\phi}}{\\\\partial \\\\hat{x}} = \\\\frac{\\\\varepsilon_{Ddl}(\\\\hat{\\\\phi} - \\\\hat{E})}{\\\\hat{d}_{Ddl}}\n",
    "\n",
    "    **Film/Solution Interface (x̂ = L̂):**\n",
    "\n",
    "    *Cation Vacancy Flux:*\n",
    "\n",
    "    .. math::\n",
    "        -D_{cv}\\\\frac{\\\\partial \\\\hat{c}_{cv}}{\\\\partial \\\\hat{x}} = \\\\left(\\\\hat{k}_3 - U_{cv}\\\\frac{\\\\partial \\\\hat{\\\\phi}}{\\\\partial \\\\hat{x}}\\\\right)\\\\hat{c}_{cv}\n",
    "\n",
    "    *Anion Vacancy Flux:*\n",
    "\n",
    "    .. math::\n",
    "        -D_{av}\\\\frac{\\\\partial \\\\hat{c}_{av}}{\\\\partial \\\\hat{x}} = \\\\left(\\\\hat{k}_4 - U_{av}\\\\frac{\\\\partial \\\\hat{\\\\phi}}{\\\\partial \\\\hat{x}}\\\\right)\\\\hat{c}_{av}\n",
    "\n",
    "    *Hole Flux:*\n",
    "\n",
    "    .. math::\n",
    "        D_h\\\\frac{\\\\partial \\\\hat{c}_h}{\\\\partial \\\\hat{x}} = \\\\hat{q}\\\\hat{c}_h\n",
    "\n",
    "    where :math:`\\\\hat{q} = -(\\\\hat{k}_{tp} + \\\\frac{FD_h}{RT}\\\\frac{\\\\partial \\\\hat{\\\\phi}}{\\\\partial \\\\hat{x}})` for :math:`\\\\hat{c}_h > 10^{-9}`\n",
    "\n",
    "    *Potential Boundary:*\n",
    "\n",
    "    .. math::\n",
    "        \\\\varepsilon_f \\\\frac{\\\\partial \\\\hat{\\\\phi}}{\\\\partial \\\\hat{x}} = \\\\varepsilon_{Ddl}\\\\hat{\\\\phi}\n",
    "    Args:\n",
    "        x: Boundary spatial coordinates\n",
    "        t: Time coordinates\n",
    "        E: Applied potential\n",
    "        networks: NetworkManager instance\n",
    "        physics: ElectrochemicalPhysics instance\n",
    "        return_residuals: If True, also return raw residuals for NTK computation\n",
    "\n",
    "    Returns:\n",
    "        If return_residuals=False: Tuple of (total_boundary_loss, individual_losses_dict)\n",
    "        If return_residuals=True: Tuple of (total_boundary_loss, individual_losses_dict, combined_residuals)\n",
    "    \"\"\"\n",
    "    batch_size = x.shape[0]\n",
    "    half_batch = batch_size // 2\n",
    "\n",
    "    # Split into metal/film and film/solution interfaces\n",
    "    x_mf = x[:half_batch]\n",
    "    x_fs = x[half_batch:]\n",
    "    t_mf = t[:half_batch]\n",
    "    t_fs = t[half_batch:]\n",
    "    E_mf = E[:half_batch]\n",
    "    E_fs = E[half_batch:]\n",
    "\n",
    "    # Predict L and compute derivative for boundary fluxes\n",
    "    L_input = torch.cat([t, E], dim=1)\n",
    "    L_pred = L_net(L_input)\n",
    "    L_pred_t = _grad(L_pred, t)\n",
    "    L_pred_t_mf = L_pred_t[:half_batch]\n",
    "\n",
    "    # Metal/film interface conditions\n",
    "    inputs_mf = torch.cat([x_mf, t_mf, E_mf], dim=1)\n",
    "    u_pred_mf = u_net(inputs_mf)\n",
    "    u_pred_mf_x = _grad(u_pred_mf, x_mf)\n",
    "\n",
    "    # CV at m/f interface\n",
    "    cv_pred_mf = cv_net(inputs_mf)\n",
    "    cv_pred_mf_x = _grad(cv_pred_mf, x_mf)\n",
    "    cv_mf_residual = ((-D_cv * cc / lc) * cv_pred_mf_x -\n",
    "                      k1_0 * torch.exp(\n",
    "                alpha_cv * phic * (E_mf / phic - u_pred_mf)) -\n",
    "                      (U_cv * phic / lc * u_pred_mf_x -\n",
    "                       lc / tc * L_pred_t_mf) * cc * cv_pred_mf)\n",
    "    cv_mf_loss = torch.mean(cv_mf_residual ** 2)\n",
    "\n",
    "    # AV at m/f interface\n",
    "    av_pred_mf = av_net(inputs_mf)\n",
    "    av_pred_mf_x = _grad(av_pred_mf, x_mf)\n",
    "    av_mf_residual = ((-D_av * cc / lc) * av_pred_mf_x -\n",
    "                      (4 / 3) * k2_0 * torch.exp(\n",
    "                alpha_av * phic * (E_mf / phic - u_pred_mf)) -\n",
    "                      (U_av * phic / lc * u_pred_mf_x -\n",
    "                       lc / tc * L_pred_t_mf) * av_pred_mf)\n",
    "    av_mf_loss = torch.mean(av_mf_residual ** 2)\n",
    "\n",
    "    # Potential at m/f interface\n",
    "    u_mf_residual = ((eps_film * phic /lc * u_pred_mf_x) -\n",
    "                     eps_Ddl * phic * (\n",
    "                             u_pred_mf - E_mf / phic) / d_Ddl)\n",
    "    u_mf_loss = torch.mean(u_mf_residual ** 2)\n",
    "\n",
    "    # Film/solution interface conditions\n",
    "    inputs_fs = torch.cat([x_fs, t_fs, E_fs], dim=1)\n",
    "    u_pred_fs = u_net(inputs_fs)\n",
    "    u_pred_fs_x = _grad(u_pred_fs, x_fs)\n",
    "\n",
    "    # CV at f/s interface\n",
    "    cv_pred_fs = cv_net(inputs_fs)\n",
    "    cv_pred_fs_x = _grad(cv_pred_fs, x_fs)\n",
    "    cv_fs_residual = ((-D_cv * cc / lc) * cv_pred_fs_x -\n",
    "                      (k3_0 * torch.exp(beta_cv * phic * u_pred_fs) -\n",
    "                       U_cv * phic / lc * u_pred_fs_x) * cv_pred_fs * cc)\n",
    "    cv_fs_loss = torch.mean(cv_fs_residual ** 2)\n",
    "\n",
    "    # AV at f/s interface\n",
    "    av_pred_fs = av_net(inputs_fs)\n",
    "    av_pred_fs_x = _grad(av_pred_fs, x_fs)\n",
    "    av_fs_residual = ((-D_av * cc / lc) * av_pred_fs_x -\n",
    "                      (k4_0 * torch.exp(alpha_av * u_pred_fs) -\n",
    "                       U_av * phic / lc * u_pred_fs_x) * av_pred_fs * cc)\n",
    "    av_fs_loss = torch.mean(av_fs_residual ** 2)\n",
    "\n",
    "    # Potential at f/s interface\n",
    "    u_fs_residual = ((eps_film * phic / lc * u_pred_fs_x) -\n",
    "                     (eps_Ddl * phic * u_pred_fs))\n",
    "    u_fs_loss = torch.mean(u_fs_residual ** 2)\n",
    "\n",
    "    # Total boundary loss\n",
    "    total_boundary_loss = cv_mf_loss + u_mf_loss + cv_fs_loss + av_fs_loss + u_fs_loss + av_mf_loss \n",
    "\n",
    "    individual_losses = {\n",
    "        'cv_mf_bc': cv_mf_loss,\n",
    "        'av_mf_bc': av_mf_loss,\n",
    "        'u_mf_bc': u_mf_loss,\n",
    "        'cv_fs_bc': cv_fs_loss,\n",
    "        'av_fs_bc': av_fs_loss,\n",
    "        'u_fs_bc': u_fs_loss,\n",
    "    }\n",
    "\n",
    "    if return_residuals:\n",
    "        # Combine all residuals into single tensor for NTK computation\n",
    "\n",
    "        residuals_dict = {'cv_mf_bc':cv_mf_residual, 'av_mf_bc':av_mf_residual, 'u_mf_bc':u_mf_residual, \n",
    "                    'cv_fs_bc':cv_fs_residual, 'av_fs_bc':av_mf_residual, 'u_fs_bc':u_fs_residual}\n",
    "        combined_residuals = torch.cat([\n",
    "            cv_mf_residual, cv_fs_residual,\n",
    "            av_mf_residual, av_fs_residual,\n",
    "        ])\n",
    "        return total_boundary_loss, individual_losses, combined_residuals,residuals_dict\n",
    "    else:\n",
    "        return total_boundary_loss, individual_losses\n",
    "\n",
    "def compute_initial_loss(x: torch.Tensor, t: torch.Tensor, E: torch.Tensor,\n",
    "                         return_residuals: bool = False) -> Union[Tuple[torch.Tensor, Dict[str, torch.Tensor]],\n",
    "Tuple[torch.Tensor, Dict[str, torch.Tensor], torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Compute initial condition losses.\n",
    "\n",
    "    **Initial Conditions (t̂ = 0):**\n",
    "\n",
    "    **Film Thickness:**\n",
    "\n",
    "    .. math::\n",
    "        \\\\hat{L}(0) = \\\\frac{L_0}{\\\\hat{L}_c}\n",
    "\n",
    "    **Cation Vacancy Concentration:**\n",
    "\n",
    "    .. math::\n",
    "        \\\\hat{c}_{cv}(\\\\hat{x}, 0) = 0\n",
    "\n",
    "    .. math::\n",
    "        \\\\frac{\\\\partial \\\\hat{c}_{cv}}{\\\\partial \\\\hat{t}}\\\\bigg|_{\\\\hat{t}=0} = 0\n",
    "\n",
    "    **Anion Vacancy Concentration:**\n",
    "\n",
    "    .. math::\n",
    "        \\\\hat{c}_{av}(\\\\hat{x}, 0) = 0\n",
    "\n",
    "    .. math::\n",
    "        \\\\frac{\\\\partial \\\\hat{c}_{av}}{\\\\partial \\\\hat{t}}\\\\bigg|_{\\\\hat{t}=0} = 0\n",
    "\n",
    "    **Potential Distribution:**\n",
    "\n",
    "    .. math::\n",
    "        \\\\hat{\\\\phi}(\\\\hat{x}, 0) = \\\\frac{\\\\hat{E}}{\\\\hat{\\\\phi}_c} - \\\\frac{10^7 \\\\hat{L}_c}{\\\\hat{\\\\phi}_c}\\\\hat{x}\n",
    "\n",
    "    .. math::\n",
    "        \\\\frac{\\\\partial \\\\hat{\\\\phi}}{\\\\partial \\\\hat{t}}\\\\bigg|_{\\\\hat{t}=0} = 0\n",
    "\n",
    "    **Hole Concentration:**\n",
    "\n",
    "    .. math::\n",
    "        \\\\hat{c}_h(\\\\hat{x}, 0) = \\\\frac{c_{h0}}{\\\\hat{c}_{h,c}} = 1\n",
    "\n",
    "    .. math::\n",
    "        \\\\frac{\\\\partial \\\\hat{c}_h}{\\\\partial \\\\hat{t}}\\\\bigg|_{\\\\hat{t}=0} = 0\n",
    "\n",
    "    Args:\n",
    "        x: Spatial coordinates\n",
    "        t: Time coordinates (should be zeros)\n",
    "        E: Applied potential\n",
    "        networks: NetworkManager instance\n",
    "        physics: ElectrochemicalPhysics instance\n",
    "        return_residuals: If True, also return raw residuals for NTK computation\n",
    "\n",
    "    Returns:\n",
    "        If return_residuals=False: Tuple of (total_initial_loss, individual_losses_dict)\n",
    "        If return_residuals=True: Tuple of (total_initial_loss, individual_losses_dict, combined_residuals)\n",
    "    \"\"\"\n",
    "    L_input = torch.cat([t, E], dim=1)\n",
    "    L_initial_pred = L_net(L_input)\n",
    "    inputs = torch.cat([x, t, E], dim=1)\n",
    "\n",
    "    # Film thickness initial condition\n",
    "    L_initial_residual = L_initial_pred - L_initial / lc\n",
    "    L_initial_loss = torch.mean(L_initial_residual ** 2)\n",
    "\n",
    "    # Cation vacancy initial conditions\n",
    "    cv_initial_pred = cv_net(inputs)\n",
    "    cv_initial_t = _grad(cv_initial_pred, t)\n",
    "    cv_initial_residual = cv_initial_pred + cv_initial_t\n",
    "    cv_initial_loss = torch.mean(cv_initial_pred**2) + torch.mean(cv_initial_t**2)\n",
    "\n",
    "    # Anion vacancy initial conditions\n",
    "    av_initial_pred = av_net(inputs)\n",
    "    av_initial_t = _grad(av_initial_pred, t)\n",
    "    av_initial_residual = av_initial_pred + av_initial_t\n",
    "    av_initial_loss = torch.mean(av_initial_pred**2) + torch.mean(av_initial_t**2)\n",
    "\n",
    "    # Potential initial conditions\n",
    "    u_initial_pred = u_net(inputs)\n",
    "    u_initial_t = _grad(u_initial_pred, t)\n",
    "    poisson_initial_residual = (u_initial_pred - (\n",
    "            (E / phic) - (1e7 * (lc / phic) * x))) + u_initial_t\n",
    "    poisson_initial_loss = torch.mean((u_initial_pred - (\n",
    "            (E / phic) - (1e7 * (lc / phic) * x)))**2) + torch.mean(u_initial_t**2)\n",
    "\n",
    "    # Total initial loss\n",
    "    total_initial_loss = cv_initial_loss + av_initial_loss + poisson_initial_loss + L_initial_loss\n",
    "\n",
    "    individual_losses = {\n",
    "        'cv_ic': cv_initial_loss,\n",
    "        'av_ic': av_initial_loss,\n",
    "        'poisson_ic': poisson_initial_loss,\n",
    "        'L_ic': L_initial_loss\n",
    "    }\n",
    "\n",
    "    if return_residuals:\n",
    "        # Combine all residuals into single tensor for NTK computation\n",
    "        residual_dict = {'cv_ic':cv_initial_residual, 'av_ic':av_initial_residual,'poisson_ic':poisson_initial_residual, 'L_ic':L_initial_residual}\n",
    "        combined_residuals = torch.cat([\n",
    "            L_initial_residual,\n",
    "            cv_initial_residual,\n",
    "            av_initial_residual,\n",
    "            poisson_initial_residual,\n",
    "        ])\n",
    "        return total_initial_loss, individual_losses, combined_residuals,residual_dict\n",
    "    else:\n",
    "        return total_initial_loss, individual_losses\n",
    "\n",
    "def compute_film_physics_loss(t: torch.Tensor, E: torch.Tensor,\n",
    "                              return_residuals: bool = False) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Compute film growth physics loss.\n",
    "\n",
    "     **Film Growth Equation:**\n",
    "\n",
    "    .. math::\n",
    "        \\\\frac{dL}{dt} = \\\\Omega (k_2 - k_5)\n",
    "\n",
    "    **Dimensionless Form:**\n",
    "\n",
    "    .. math::\n",
    "        \\\\frac{d\\\\hat{L}}{d\\\\hat{t}} = \\\\frac{\\\\hat{t}_c \\\\Omega}{\\\\hat{L}_c} (k_2 - k_5)\n",
    "\n",
    "\n",
    "    Args:\n",
    "        t: Time coordinates\n",
    "        E: Applied potential\n",
    "        networks: NetworkManager instance\n",
    "        physics: ElectrochemicalPhysics instance\n",
    "        return_residuals: If True, also return raw residuals for NTK computation\n",
    "\n",
    "    Returns:\n",
    "        If return_residuals=False: Film physics loss tensor\n",
    "        If return_residuals=True: Tuple of (film_physics_loss, residuals)\n",
    "    \"\"\"\n",
    "    inputs = torch.cat([t, E], dim=1)\n",
    "    L_pred = L_net(inputs)\n",
    "\n",
    "    # Get rate constants\n",
    "    k1, k2, k3, k4, k5 = compute_rate_constants(t, E)\n",
    "\n",
    "    # Compute predicted and physics-based dL/dt\n",
    "    dl_dt_pred = _grad(L_pred, t)\n",
    "    dL_dt_physics = (1 /lc) * tc * Omega * (k2 - k5) #k_2 being problematic, we see this in polarization curve would make sense if it happens here too\n",
    "\n",
    "    # Compute residual\n",
    "    film_residual = dl_dt_pred - dL_dt_physics\n",
    "    film_loss = torch.mean(film_residual ** 2)\n",
    "\n",
    "    if return_residuals:\n",
    "        return film_loss, film_residual\n",
    "    else:\n",
    "        return film_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b9e9c830",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get total loss with the NTK strategy\n",
    "optimal_batch_sizes = {}\n",
    "from typing import List\n",
    "def compute_jacobian(\n",
    "        output: torch.Tensor,\n",
    "        parameters: List[torch.nn.Parameter],\n",
    "        device: torch.device\n",
    ") -> torch.Tensor:\n",
    "        \"\"\"\n",
    "    Compute Jacobian matrix using fast batched gradient computation.\n",
    "\n",
    "    Args:\n",
    "        outputs: Network outputs [batch_size] or [batch_size, 1]\n",
    "        parameters: List of network parameters\n",
    "        device: PyTorch device\n",
    "\n",
    "    Returns:\n",
    "        Jacobian matrix \n",
    "    \"\"\"\n",
    "        output = output.reshape(-1)\n",
    "        grads = torch.autograd.grad(\n",
    "            output,\n",
    "            list(parameters),\n",
    "            (torch.eye(output.shape[0]).to(device),),\n",
    "            is_grads_batched=True, retain_graph=True,allow_unused=True\n",
    "        )\n",
    "        valid_grads = [grad.flatten().reshape(len(output), -1) \n",
    "                   for grad in grads if grad is not None]\n",
    "        \n",
    "        return torch.cat(valid_grads, 1)\n",
    "\n",
    "def get_ntk(jac:torch.Tensor\n",
    "            ,compute=\"trace\") -> torch.Tensor:\n",
    "    \"\"\"Get the NTK matrix of jac \"\"\"\n",
    "\n",
    "    if compute == 'full':\n",
    "        return torch.einsum('Na,Ma->NM', jac, jac)\n",
    "    elif compute == 'diag':\n",
    "        return torch.einsum('Na,Na->N', jac, jac)\n",
    "    elif compute == 'trace':\n",
    "        return torch.einsum('Na,Na->', jac, jac)\n",
    "    else:\n",
    "        raise ValueError('compute must be one of \"full\",'\n",
    "                            + '\"diag\", or \"trace\"')\n",
    "\n",
    "def compute_minimum_batch_size(jacobian):\n",
    "    \"\"\"Compute minimum batch size for 0.2 approximation error\"\"\"\n",
    "    ntk_diag = get_ntk(jacobian, compute='diag')\n",
    "    # Population statistics\n",
    "    mu_X = torch.mean(ntk_diag)\n",
    "    sigma_X = torch.std(ntk_diag)\n",
    "    \n",
    "    # Handle near-zero mean case\n",
    "    if mu_X.abs() < 1e-8:\n",
    "        # Use relative variation instead when mean is tiny\n",
    "        if sigma_X < 1e-8:\n",
    "            v_X = 1.0  # Uniform case\n",
    "        else:\n",
    "            # Use median as reference instead of mean\n",
    "            median_X = torch.median(ntk_diag)\n",
    "            v_X = sigma_X / (median_X.abs() + 1e-8)\n",
    "    else:\n",
    "        # Normal coefficient of variation\n",
    "        v_X = sigma_X / mu_X.abs()\n",
    "    \n",
    "    # Clamp to reasonable bounds\n",
    "    v_X = torch.clamp(v_X, min=0.1, max=5.0)\n",
    "    \n",
    "    min_batch_size = int(25 * (v_X ** 2))\n",
    "    min_batch_size = max(min_batch_size, 32)\n",
    "    min_batch_size = min(min_batch_size, len(jacobian) // 4)\n",
    "    \n",
    "    return min_batch_size\n",
    "\n",
    "def compute_ntk_trace(\n",
    "        loss_residuals: torch.Tensor,\n",
    "        loss_name: str\n",
    ") -> Tuple[float, int]:\n",
    "    \"\"\"\n",
    "    Compute NTK-based weight for a single loss component.\n",
    "\n",
    "    Args:\n",
    "        loss_residuals: Residual tensor for this loss [batch_size]\n",
    "        loss_name: Name of loss component\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (ntk_trace, effective_batch_size)\n",
    "    \"\"\"\n",
    "    # Determine batch size (one-time calculation)\n",
    "    if loss_name not in optimal_batch_sizes:\n",
    "        indices = torch.randperm(len(loss_residuals),device=device)[:256]\n",
    "        residual_sampled = loss_residuals[indices]\n",
    "        jacobian_sampled = compute_jacobian(residual_sampled,total_model_parameters,device)\n",
    "        optimal_batch_sizes[loss_name] = compute_minimum_batch_size(jacobian_sampled)\n",
    "        print(f\"Computed batch size for {loss_name}: {optimal_batch_sizes[loss_name]}\")\n",
    "\n",
    "        trace = get_ntk(jacobian_sampled, compute='trace')\n",
    "        \n",
    "        return trace, len(jacobian_sampled) \n",
    "\n",
    "    #Use computed optimal batch size every other time\n",
    "    else:\n",
    "        # Random sampling\n",
    "        batch_size = optimal_batch_sizes[loss_name]\n",
    "        indices = torch.randperm(len(loss_residuals),device=device)[:batch_size]\n",
    "        residual_sampled = loss_residuals[indices]\n",
    "        jacobian_sampled = compute_jacobian(residual_sampled,total_model_parameters,device)\n",
    "\n",
    "        # Compute NTK trace\n",
    "        trace = get_ntk(jacobian_sampled, compute='trace')\n",
    "        \n",
    "        return trace, len(jacobian_sampled)\n",
    "    \n",
    "def extract_all_residuals() -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Extract all residuals using the modified loss functions with return_residuals=True.\n",
    "\n",
    "    This leverages the exact same computation logic as training without duplication.\n",
    "    \"\"\"\n",
    "    # Sample training points (same as training)\n",
    "    x_interior, t_interior, E_interior = sample_interior_points()\n",
    "    x_boundary, t_boundary, E_boundary = sample_boundary_points()\n",
    "    x_initial, t_initial, E_initial = sample_initial_points()\n",
    "    t_film, E_film = sample_film_physics_points()\n",
    "\n",
    "    # Extract residuals using existing loss functions\n",
    "    _, _, interior_residuals = compute_interior_loss(\n",
    "        x_interior, t_interior, E_interior,\n",
    "        return_residuals=True\n",
    "    )\n",
    "\n",
    "    _, _, boundary_residuals, _ = compute_boundary_loss(\n",
    "        x_boundary, t_boundary, E_boundary,\n",
    "        return_residuals=True\n",
    "    )\n",
    "\n",
    "    _, _, initial_residuals, _ = compute_initial_loss(\n",
    "        x_initial, t_initial, E_initial,\n",
    "        return_residuals=True\n",
    "    )\n",
    "\n",
    "    _, film_residuals = compute_film_physics_loss(\n",
    "        t_film, E_film,\n",
    "        return_residuals=True\n",
    "    )\n",
    "\n",
    "    # Combine all residuals\n",
    "    all_residuals = {\n",
    "        **interior_residuals,  # cv_pde, av_pde, poisson_pde\n",
    "        'boundary': boundary_residuals,\n",
    "        'initial': initial_residuals,\n",
    "        'film_physics': film_residuals\n",
    "    }\n",
    "\n",
    "    return all_residuals\n",
    "\n",
    "def compute_weights() -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute NTK weights using exact residuals from existing loss computations.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of normalized weights\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract all residuals using modified loss functions\n",
    "    all_residuals = extract_all_residuals()\n",
    "\n",
    "    ntk_traces = {}\n",
    "    batch_sizes = {}\n",
    "    \n",
    "    # Compute NTK trace for each component\n",
    "    for component_name, residual in all_residuals.items():\n",
    "        if len(residual) > 0:\n",
    "            ntk_trace, effective_batch_size = compute_ntk_trace(residual, component_name)\n",
    "            ntk_traces[component_name] = ntk_trace\n",
    "            batch_sizes[component_name] = effective_batch_size\n",
    "\n",
    "\n",
    "    # Compute normalized weights\n",
    "    weights = _normalize_ntk_weights(ntk_traces, batch_sizes)\n",
    "\n",
    "    return weights\n",
    "\n",
    "def _normalize_ntk_weights(\n",
    "        ntk_traces: Dict[str, float],\n",
    "        batch_sizes: Dict[str, int]\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Normalize NTK traces to get balanced weights.\n",
    "\n",
    "    Args:\n",
    "        ntk_traces: Dictionary of NTK traces\n",
    "        batch_sizes: Dictionary of batch sizes\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of normalized weights\n",
    "    \"\"\"\n",
    "    # Compute mean traces (trace per sample)\n",
    "    mean_traces = {}\n",
    "    for name in ntk_traces:\n",
    "        mean_traces[name] = ntk_traces[name] / batch_sizes[name]\n",
    "\n",
    "    # Compute raw weights\n",
    "    raw_weights = {}\n",
    "    for name, mean_trace in mean_traces.items():\n",
    "        if mean_trace > 1e-12:  # Avoid division by zero\n",
    "            sum_all_mean_traces = sum(mean_traces[n] for n,_ in mean_traces.items())\n",
    "            raw_weights[name] = 1.0 / mean_trace * sum_all_mean_traces\n",
    "        else:\n",
    "            raw_weights[name] = 1.0\n",
    "\n",
    "    # Normalize weights\n",
    "    total_raw_weight = sum(raw_weights.values())\n",
    "    normalization = len(raw_weights)/total_raw_weight\n",
    "    \n",
    "    normalized_weights = {\n",
    "        name: raw_weights[name] * normalization\n",
    "        for name, weight in raw_weights.items()\n",
    "    }\n",
    "\n",
    "\n",
    "def total_loss():\n",
    "     #Investigate using one E for all the different samples, this might help a lot. Could do an easy fix here\n",
    "    x_interior, t_interior, E_interior = sample_interior_points()\n",
    "    x_boundary, t_boundary, E_boundary = sample_boundary_points()\n",
    "    x_inital, t_initial, E_initial = sample_initial_points()\n",
    "    t_film, E_film = sample_film_physics_points()\n",
    "\n",
    "    total_interior_loss, individual_losses, residuals_interior = compute_interior_loss(x_interior,t_interior,E_interior,return_residuals=True)\n",
    "    total_boundary_loss, boundary_losses, combined_residuals_bc,residuals_dict_bc = compute_boundary_loss(x_boundary,t_boundary,E_boundary,return_residuals=True)\n",
    "    total_initial_loss, initial_losses, combined_residuals_ic,residuals_dict_ic = compute_initial_loss(x_inital,t_initial,E_initial,return_residuals=True)\n",
    "    film_loss, film_residual = compute_film_physics_loss(t_film,E_film,return_residuals=True)\n",
    "\n",
    "    weights = compute_weights()\n",
    "    \n",
    "    loss = weights['cv_pde']*individual_losses['cv_pde'] + weights['av_pde']*individual_losses['av_pde'] + weights['poisson_pde']*individual_losses['poisson_pde'] + weights['boundary']*total_boundary_loss + weights['initial']*total_initial_loss + weights['film_physics']*film_loss \n",
    "    \n",
    "    interior_loss = weights['cv_pde']*individual_losses['cv_pde'] + weights['av_pde']*individual_losses['av_pde'] + weights['poisson_pde']*individual_losses['poisson_pde']\n",
    "    return loss, interior_loss, weights['boundary']*total_boundary_loss,weights['initial']*total_initial_loss, weights['film_physics']*film_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e5cc647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44d0ebd40b7c41e1a316cae0b656fc6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Status:   0%|          | 0/12000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed batch size for cv_pde: 32\n",
      "Computed batch size for av_pde: 32\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m L_net.train()\n\u001b[32m     15\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m loss, interior_loss, boundary_loss, initial_loss, film_loss = \u001b[43mtotal_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m loss_total = interior_loss + boundary_loss + initial_loss + film_loss\n\u001b[32m     19\u001b[39m loss.backward()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 234\u001b[39m, in \u001b[36mtotal_loss\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    231\u001b[39m total_initial_loss, initial_losses, combined_residuals_ic,residuals_dict_ic = compute_initial_loss(x_inital,t_initial,E_initial,return_residuals=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    232\u001b[39m film_loss, film_residual = compute_film_physics_loss(t_film,E_film,return_residuals=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m weights = \u001b[43mcompute_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    236\u001b[39m loss = weights[\u001b[33m'\u001b[39m\u001b[33mcv_pde\u001b[39m\u001b[33m'\u001b[39m]*individual_losses[\u001b[33m'\u001b[39m\u001b[33mcv_pde\u001b[39m\u001b[33m'\u001b[39m] + weights[\u001b[33m'\u001b[39m\u001b[33mav_pde\u001b[39m\u001b[33m'\u001b[39m]*individual_losses[\u001b[33m'\u001b[39m\u001b[33mav_pde\u001b[39m\u001b[33m'\u001b[39m] + weights[\u001b[33m'\u001b[39m\u001b[33mpoisson_pde\u001b[39m\u001b[33m'\u001b[39m]*individual_losses[\u001b[33m'\u001b[39m\u001b[33mpoisson_pde\u001b[39m\u001b[33m'\u001b[39m] + weights[\u001b[33m'\u001b[39m\u001b[33mboundary\u001b[39m\u001b[33m'\u001b[39m]*total_boundary_loss + weights[\u001b[33m'\u001b[39m\u001b[33minitial\u001b[39m\u001b[33m'\u001b[39m]*total_initial_loss + weights[\u001b[33m'\u001b[39m\u001b[33mfilm_physics\u001b[39m\u001b[33m'\u001b[39m]*film_loss \n\u001b[32m    238\u001b[39m interior_loss = weights[\u001b[33m'\u001b[39m\u001b[33mcv_pde\u001b[39m\u001b[33m'\u001b[39m]*individual_losses[\u001b[33m'\u001b[39m\u001b[33mcv_pde\u001b[39m\u001b[33m'\u001b[39m] + weights[\u001b[33m'\u001b[39m\u001b[33mav_pde\u001b[39m\u001b[33m'\u001b[39m]*individual_losses[\u001b[33m'\u001b[39m\u001b[33mav_pde\u001b[39m\u001b[33m'\u001b[39m] + weights[\u001b[33m'\u001b[39m\u001b[33mpoisson_pde\u001b[39m\u001b[33m'\u001b[39m]*individual_losses[\u001b[33m'\u001b[39m\u001b[33mpoisson_pde\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 174\u001b[39m, in \u001b[36mcompute_weights\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m component_name, residual \u001b[38;5;129;01min\u001b[39;00m all_residuals.items():\n\u001b[32m    173\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(residual) > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m         ntk_trace, effective_batch_size = \u001b[43mcompute_ntk_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomponent_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m         ntk_traces[component_name] = ntk_trace\n\u001b[32m    176\u001b[39m         batch_sizes[component_name] = effective_batch_size\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 93\u001b[39m, in \u001b[36mcompute_ntk_trace\u001b[39m\u001b[34m(loss_residuals, loss_name)\u001b[39m\n\u001b[32m     91\u001b[39m indices = torch.randperm(\u001b[38;5;28mlen\u001b[39m(loss_residuals),device=device)[:\u001b[32m256\u001b[39m]\n\u001b[32m     92\u001b[39m residual_sampled = loss_residuals[indices]\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m jacobian_sampled = \u001b[43mcompute_jacobian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresidual_sampled\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtotal_model_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m optimal_batch_sizes[loss_name] = compute_minimum_batch_size(jacobian_sampled)\n\u001b[32m     95\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mComputed batch size for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimal_batch_sizes[loss_name]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mcompute_jacobian\u001b[39m\u001b[34m(output, parameters, device)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[33;03mCompute Jacobian matrix using fast batched gradient computation.\u001b[39;00m\n\u001b[32m     11\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m \u001b[33;03m    Jacobian matrix \u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     20\u001b[39m     output = output.reshape(-\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     grads = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43meye\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m     valid_grads = [grad.flatten().reshape(\u001b[38;5;28mlen\u001b[39m(output), -\u001b[32m1\u001b[39m) \n\u001b[32m     28\u001b[39m                \u001b[38;5;28;01mfor\u001b[39;00m grad \u001b[38;5;129;01min\u001b[39;00m grads \u001b[38;5;28;01mif\u001b[39;00m grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m     30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat(valid_grads, \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/physicsOxy/lib/python3.11/site-packages/torch/autograd/__init__.py:492\u001b[39m, in \u001b[36mgrad\u001b[39m\u001b[34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[39m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvjp\u001b[39m(gO):\n\u001b[32m    482\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _engine_run_backward(\n\u001b[32m    483\u001b[39m             outputs,\n\u001b[32m    484\u001b[39m             gO,\n\u001b[32m   (...)\u001b[39m\u001b[32m    489\u001b[39m             accumulate_grad=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    490\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m492\u001b[39m     result = \u001b[43m_vmap_internals\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_vmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvjp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_none_pass_through\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    496\u001b[39m     result = _engine_run_backward(\n\u001b[32m    497\u001b[39m         outputs,\n\u001b[32m    498\u001b[39m         grad_outputs_,\n\u001b[32m   (...)\u001b[39m\u001b[32m    503\u001b[39m         accumulate_grad=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    504\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/physicsOxy/lib/python3.11/site-packages/torch/_vmap_internals.py:231\u001b[39m, in \u001b[36m_vmap.<locals>.wrapped\u001b[39m\u001b[34m(*args)\u001b[39m\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    228\u001b[39m     batched_inputs, batch_size = _create_batched_inputs(\n\u001b[32m    229\u001b[39m         in_dims, args, vmap_level, func\n\u001b[32m    230\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m     batched_outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    232\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_none_pass_through:\n\u001b[32m    233\u001b[39m         _validate_outputs(batched_outputs, func)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/physicsOxy/lib/python3.11/site-packages/torch/autograd/__init__.py:482\u001b[39m, in \u001b[36mgrad.<locals>.vjp\u001b[39m\u001b[34m(gO)\u001b[39m\n\u001b[32m    481\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvjp\u001b[39m(gO):\n\u001b[32m--> \u001b[39m\u001b[32m482\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgO\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/physicsOxy/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#main training loop\n",
    "\n",
    "optimizer=torch.optim.Adam([{'params': total_model_parameters}],lr=lr)\n",
    "loss_list = []\n",
    "interior_loss_list = []\n",
    "boundary_loss_list = []\n",
    "initial_loss_list = []\n",
    "film_loss_list = []\n",
    "\n",
    "for step in tqdm(range(max_steps),desc=\"Training Status\"):\n",
    "    cv_net.train()\n",
    "    av_net.train()\n",
    "    u_net.train()\n",
    "    L_net.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss, interior_loss, boundary_loss, initial_loss, film_loss = total_loss()\n",
    "    loss_total = interior_loss + boundary_loss + initial_loss + film_loss\n",
    "    loss.backward()\n",
    "\n",
    "\n",
    "    optimizer.step()\n",
    "        \n",
    "    loss_list.append(loss_total.item())\n",
    "    interior_loss_list.append(interior_loss.item())\n",
    "    boundary_loss_list.append(boundary_loss.item())\n",
    "    initial_loss_list.append(initial_loss.item())\n",
    "    film_loss_list.append(film_loss.item())\n",
    "\n",
    "    if step % print_freq == 0:\n",
    "        tqdm.write(f\"Total:{loss_total}, interior:{interior_loss}, boundary:{boundary_loss}, initial:{initial_loss}, film:{film_loss} at step:{step}, lamdbda sign:{torch.sign(l_cv_mf.grad)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757334eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "window=100\n",
    "# Raw loss\n",
    "plt.plot(loss_list, alpha=0.3, color='lightblue', label='Raw Loss')\n",
    "\n",
    "# Moving average for smoother trend\n",
    "if len(loss_list) > window:\n",
    "    moving_avg = np.convolve(loss_list, np.ones(window)/window, mode='valid')\n",
    "    plt.plot(range(window-1, len(loss_list)), moving_avg, \n",
    "        color='red', linewidth=2, label=f'Moving Avg ({window})')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Training Step', fontsize=12)\n",
    "plt.ylabel('Total Loss (log scale)', fontsize=12)\n",
    "plt.title('PINNACLE Training Loss Evolution', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Define input ranges (all dimensionless)\n",
    "    n_spatial = 50\n",
    "    n_temporal = 50\n",
    "\n",
    "    # Fix a representative dimensionless potential for visualization\n",
    "    E_hat_fixed = torch.tensor([[0.8 / phic]], device=device)  # Normalized E\n",
    "\n",
    "    # Dimensionless time range (0 to 1)\n",
    "    t_hat_range = torch.linspace(0, 1.0, n_temporal, device=device)\n",
    "\n",
    "    # Get final dimensionless film thickness to set spatial range\n",
    "    t_hat_final = torch.tensor([[1.0]], device=device)\n",
    "    L_hat_final = L_net(torch.cat([t_hat_final, E_hat_fixed], dim=1)).item()\n",
    "    x_hat_range = torch.linspace(0, L_hat_final, n_spatial, device=device)\n",
    "\n",
    "    print(f\"  📐 Dimensionless time range: [0, 1.0]\")\n",
    "    print(f\"  📐 Dimensionless spatial range: [0, {L_hat_final:.2f}]\")\n",
    "    print(f\"  📐 Fixed dimensionless potential: {E_hat_fixed.item():.3f}\")\n",
    "\n",
    "    # Create 2D grid for contour plots\n",
    "    T_hat_mesh, X_hat_mesh = torch.meshgrid(t_hat_range, x_hat_range, indexing='ij')\n",
    "    E_hat_mesh = torch.full_like(T_hat_mesh, E_hat_fixed.item())\n",
    "\n",
    "    # Stack inputs for 3D networks\n",
    "    inputs_3d = torch.stack([\n",
    "    X_hat_mesh.flatten(),\n",
    "    T_hat_mesh.flatten(),\n",
    "    E_hat_mesh.flatten()\n",
    "    ], dim=1)\n",
    "\n",
    "    # Get network predictions\n",
    "    phi_hat_2d = u_net(inputs_3d).reshape(n_temporal, n_spatial)\n",
    "    c_cv_hat_2d = cv_net(inputs_3d).reshape(n_temporal, n_spatial)\n",
    "    c_av_hat_2d = av_net(inputs_3d).reshape(n_temporal, n_spatial)\n",
    "\n",
    "    # Film thickness evolution (dimensionless)\n",
    "    t_hat_1d = t_hat_range.unsqueeze(1)\n",
    "    E_hat_1d = torch.full_like(t_hat_1d, E_hat_fixed.item())\n",
    "    L_inputs_1d = torch.cat([t_hat_1d, E_hat_1d], dim=1)\n",
    "    L_hat_1d = L_net(L_inputs_1d).squeeze()\n",
    "\n",
    "    # Convert to numpy for plotting\n",
    "    t_hat_np = t_hat_range.cpu().numpy()\n",
    "    x_hat_np = x_hat_range.cpu().numpy()\n",
    "    T_hat_np, X_hat_np = np.meshgrid(t_hat_np, x_hat_np, indexing='ij')\n",
    "\n",
    "    phi_hat_np = phi_hat_2d.cpu().numpy()\n",
    "    c_cv_hat_np = c_cv_hat_2d.cpu().numpy()\n",
    "    c_av_hat_np = c_av_hat_2d.cpu().numpy()\n",
    "    L_hat_np = L_hat_1d.cpu().numpy()\n",
    "\n",
    "    # Create plots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "    # 1. Dimensionless potential field\n",
    "    im1 = axes[0, 0].contourf(X_hat_np, T_hat_np, phi_hat_np, levels=50, cmap='RdBu_r')\n",
    "    axes[0, 0].set_xlabel('Dimensionless Position x̂')\n",
    "    axes[0, 0].set_ylabel('Dimensionless Time t̂')\n",
    "    axes[0, 0].set_title(f'Dimensionless Potential φ̂(x̂,t̂) at Ê={E_hat_fixed.item():.3f}')\n",
    "    plt.colorbar(im1, ax=axes[0, 0])\n",
    "\n",
    "    # 2. Dimensionless cation vacancies\n",
    "    im2 = axes[0, 1].contourf(X_hat_np, T_hat_np, c_cv_hat_np, levels=50, cmap='Reds')\n",
    "    axes[0, 1].set_xlabel('Dimensionless Position x̂')\n",
    "    axes[0, 1].set_ylabel('Dimensionless Time t̂')\n",
    "    axes[0, 1].set_title(f'Dimensionless Cation Vacancies ĉ_cv at Ê={E_hat_fixed.item():.3f}')\n",
    "    plt.colorbar(im2, ax=axes[0, 1])\n",
    "\n",
    "    # 3. Dimensionless anion vacancies\n",
    "    im3 = axes[0, 2].contourf(X_hat_np, T_hat_np, c_av_hat_np, levels=50, cmap='Blues')\n",
    "    axes[0, 2].set_xlabel('Dimensionless Position x̂')\n",
    "    axes[0, 2].set_ylabel('Dimensionless Time t̂')\n",
    "    axes[0, 2].set_title(f'Dimensionless Anion Vacancies ĉ_av at Ê={E_hat_fixed.item():.3f}')\n",
    "    plt.colorbar(im3, ax=axes[0, 2])\n",
    "\n",
    "    # 5. Dimensionless film thickness\n",
    "    axes[1, 1].plot(t_hat_np, L_hat_np, 'k-', linewidth=3)\n",
    "    axes[1, 1].set_xlabel('Dimensionless Time t̂')\n",
    "    axes[1, 1].set_ylabel('Dimensionless Film Thickness L̂')\n",
    "    axes[1, 1].set_title(f'Dimensionless Film Thickness L̂(t̂) at Ê={E_hat_fixed.item():.3f}')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # 6. Potential profile vs spatial position at fixed time\n",
    "    x_hat_sweep = torch.linspace(0, L_hat_final, 50, device=device)\n",
    "    t_hat_mid = torch.full((50, 1), 0.5, device=device)  # Middle time\n",
    "    E_hat_mid = torch.full((50, 1), E_hat_fixed.item(), device=device)\n",
    "\n",
    "    x_sweep_inputs = torch.cat([x_hat_sweep.unsqueeze(1), t_hat_mid, E_hat_mid], dim=1)\n",
    "    phi_vs_x = u_net(x_sweep_inputs).cpu().numpy()\n",
    "\n",
    "    axes[1, 2].plot(x_hat_sweep.cpu().numpy(), phi_vs_x, 'r-', linewidth=2)\n",
    "    axes[1, 2].set_xlabel('Dimensionless Position x̂')\n",
    "    axes[1, 2].set_ylabel('Dimensionless Potential φ̂')\n",
    "    axes[1, 2].set_title(f'Potential Profile φ̂(x̂) at t̂=0.5, Ê={E_hat_fixed.item():.3f}')\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.suptitle(f'Dimensionless Network Predictions Overview - Step {step}', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "physicsOxy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
